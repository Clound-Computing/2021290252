C:\Users\xueqi0819\AppData\Local\Programs\Python\Python311\python.exe C:\Users\xueqi0819\Desktop\ktnmt-master-张雪琦2021290252\ktnmt-master张雪琦2021290252\src\scripts\spm_train.py 
['botchan.txt', 'data.txt', 'getVocab.py', 'load_model.py', 'm.model', 'm.vocab', 'm1.model', 'm1.vocab', 'm2.model', 'm2.vocab', 'm3.model', 'm3.vocab', 'overwrite_vocab.py', 'processData.py', 'spm_encode.py', 'spm_train.py', 'tSNE.py', 'tsneutil.py', 'wol.dev', '__pycache__']
sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./botchan.txt --model_prefix=m1 --vocab_size=500
sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : 
trainer_spec {
  input: ./botchan.txt
  input_format: 
  model_prefix: m1
  model_type: UNIGRAM
  vocab_size: 500
  self_test_sample_size: 0
  character_coverage: 0.9995
  input_sentence_size: 0
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 0
  pretokenization_delimiter: 
  treat_whitespace_as_suffix: 0
  allow_whitespace_only_pieces: 0
  required_chars: 
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  seed_sentencepieces_file: 
  hard_vocab_limit: 1
  use_all_vocab: 0
  unk_id: 0
  bos_id: 1
  eos_id: 2
  pad_id: -1
  unk_piece: <unk>
  bos_piece: <s>
  eos_piece: </s>
  pad_piece: <pad>
  unk_surface:  ⁇ 
  enable_differential_privacy: 0
  differential_privacy_noise_level: 0
  differential_privacy_clipping_threshold: 0
}
normalizer_spec {
  name: nmt_nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}
denormalizer_spec {}
trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(185) LOG(INFO) Loading corpus: ./botchan.txt
trainer_interface.cc(409) LOG(INFO) Loaded all 4288 sentences
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>
trainer_interface.cc(430) LOG(INFO) Normalizing sentences...
trainer_interface.cc(539) LOG(INFO) all chars count=274252
trainer_interface.cc(550) LOG(INFO) Done: 99.957% characters are covered.
trainer_interface.cc(560) LOG(INFO) Alphabet size=69
trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99957
trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4288 sentences.
unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=144687
unigram_model_trainer.cc(312) LOG(INFO) Initialized 16112 seed sentencepieces
trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4288
trainer_interface.cc(609) LOG(INFO) Done! 9165
unigram_model_trainer.cc(602) LOG(INFO) Using 9165 sentences for EM training
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71845 num_tokens=20449 num_tokens/piece=5.21259
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3921 obj=8.66278 num_tokens=20450 num_tokens/piece=5.21551
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2940 obj=8.96602 num_tokens=22797 num_tokens/piece=7.75408
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2940 obj=8.88624 num_tokens=22798 num_tokens/piece=7.75442
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2205 obj=9.2744 num_tokens=25530 num_tokens/piece=11.5782
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2205 obj=9.18615 num_tokens=25533 num_tokens/piece=11.5796
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1653 obj=9.6456 num_tokens=28789 num_tokens/piece=17.4162
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1653 obj=9.55867 num_tokens=28792 num_tokens/piece=17.418
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1239 obj=10.1208 num_tokens=32288 num_tokens/piece=26.0597
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1239 obj=10.0257 num_tokens=32288 num_tokens/piece=26.0597
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=929 obj=10.6049 num_tokens=35498 num_tokens/piece=38.211
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=929 obj=10.5054 num_tokens=35498 num_tokens/piece=38.211
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=696 obj=11.0822 num_tokens=38632 num_tokens/piece=55.5057
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=696 obj=10.9787 num_tokens=38634 num_tokens/piece=55.5086
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=550 obj=11.4617 num_tokens=40909 num_tokens/piece=74.38
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=550 obj=11.3802 num_tokens=40909 num_tokens/piece=74.38
trainer_interface.cc(687) LOG(INFO) Saving model: m1.model
trainer_interface.cc(699) LOG(INFO) Saving vocabs: m1.vocab

进程已结束，退出代码为 0
