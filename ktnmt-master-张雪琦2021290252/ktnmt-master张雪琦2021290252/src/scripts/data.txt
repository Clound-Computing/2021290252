数据集详细信息
我们使用6个语言对来训练包含12个翻译方向和7种语言的原始MNMT模型(WMT-7)。所有原始训练数据均来自于最新的WMT通用翻译轨道。我们从WMT新闻翻译轨道和FLoRes进行了8个增量语言对的增量学习。所有数据都遵循可自由用于研究目的的许可(Farhad et al.， 2021)。FLoRes数据集的许可为CC-BY-SA 4.0。此外，我们按照Fan et al.(2021)对训练样本进行清洗。我们引入不同语言的特点来分析语言的多样性，如表9所示。所有语言对均以英语为中心，训练数据统计如表10所示。
A.1数据统计
作为一般设置，所有语言对按照并行数据量分为三类，包括高资源(>10M)、中等资源(1M~10M)和低资源(100k~1M)。具体来说，原始语言对是，高资源:日语和波兰语;媒介资源:冰岛语和普什图语;资源匮乏:豪萨语和泰米尔语。增加的语言对是:中等资源:德语和孟加拉语;资源匮乏:乌克兰和罗马尼亚。请注意，增量训练数据在现实场景中通常不是高资源。
A.2语言考虑
在这项工作中，考虑到增量语言的多样性，我们探索了增量学习中MNMT更复杂和全面的场景。这些增量语言在文字上与原始语言不同，属于不同的语系，这导致了严重的词汇和语言差距。受Zhang等人(2022)的启发，如果增量语言与原始语言集具有不同的脚本，则在原始语言和增量语言之间会出现一定比例的语义不明确的OOV令牌，从而阻碍在新语言对上的性能。此外，重要的是要注意，语族是指一组具有共同祖先的语言，称为原始语言。这一概念强调了语言之间的历史联系及其随时间的演变。此外，在不同的语系中也可以观察到语法和词序的差异。这些语言差异进一步加剧了增量语言之间的差距，使其翻译更具挑战性。
在我们的设置中，4种增量语言包括:孟加拉语，它与原始7种语言中的任何一种都没有关系，并且具有独特的脚本;乌克兰语，与原始语言波兰语有关，属于斯拉夫语系，但有一个独特的西里尔字母;罗马尼亚语是一种罗曼语，与所有原始语言都没有关系，但与拉丁字符有共同的脚本;德语，在语族和文字上与原始语言相似。原始语言和增量语言的数据集统计和详细情况见表9。
B型号详细信息
B.1培训设置
我们在所有的实验中都实现了Transformer转换模型。特别是，小原始模型(0.4B)由6个堆叠的编码器层、6个堆叠的解码器层和16个多关注头组成，其次是Transformer-Big的配置(Vaswanietal.，2017)。d model和d ffn的维数分别为1024和4096。大型原始模型(1.2B)由24层堆叠的编码器层、24层堆叠的解码器层和16个多关注磁头组成，然后配置M2M-100 (Fan et al.， 2021)。d model和d ffn的维数分别为1024和8192。我们使用Adam (Kingma and Ba, 2014)和半精度训练方案对所有MNMT模型的参数进行优化。此外，我们重置了增量学习中的优化器和学习调度程序，并使用温度为T = 5的基于温度的采样方案(Arivazhagan et al.， 2019)来平衡不同语言对之间的训练数据。我们在增量学习中采用早停(耐心为10)策略，所有训练过程的批大小为4096 × 4。为了消除结果的随机性，我们报告了在五个种子中训练的模型的平均BLEU分数。所有增量模型均在2个NVIDIA A100 gpu上进行训练。
B.2持续学习基线
我们将我们的方法与持续学习中的各种代表性基线进行了比较。基线如下:
•Replay (Sun et al.， 2019):为原始语言对创建伪数据，并使用伪数据和增量训练数据联合训练新的语言对。
•EWC (Kirkpatrick et al.， 2017):使用Fisher矩阵计算参数的重要性，并在损失函数中使用额外的惩罚来保留原始知识。
•Self-KD (Castellucci et al.， 2021):利用原始模型作为教师模型来提炼旧知识。
•LFR (Gu et al.， 2022):约束低遗忘风险区域原始模型的参数。我们选择LRF-CM来适应新的语言对。
提示(Chalkidis et al.， 2021):将提示添加到第一层的输入嵌入中。
•前缀(Li and Liang, 2021):在每一层关注的键和值前添加前缀。
C更多的比较
C.1培训费用
为了进一步说明我们方法的效率，我们将训练时间与更强的基线进行比较，如图3所示。结果表明，知识迁移方法可以减少增量学习的训练时间，比其他方法更有效和实用。
C.2多语言表示的可视化
如图4所示，我们将xx-to-English翻译方向上的句子表示可视化，以研究语言之间的表示差距。由于在一个表示空间中的可比性，我们需要在不同语言中表示相同含义的多源句子。
我们使用“FLoRes”并使用t-SNE将1024暗表示减少到2暗(Van der Maaten和Hinton, 2008)进行可视化。如图4所示，使用我们方法的句子表示比标准Adapter方法(其中一个基线)更接近。这表明我们的方法能很好地适应新语言。此外，已有研究表明，语义相似的句子在表示空间中距离越近，通常可以提高零射击翻译的翻译性能。在两个翻译方向上的实验结果表明，我们的方法可以获得更好的零射击翻译性能，这与我们的可视化结果一致。
C.3案例分析
我们提供了几个翻译示例，以提供对知识转移方法的全面理解，如表12所示。实例表明，该方法能够有效地适应新语言，特别是当增量语言与原始语言集不相关时。特别是，由于词汇量的差距，适配器方法很容易学习具有与拉丁语不同脚本的增量语言。虽然Extension通过扩展嵌入层来缓解这个问题，但是对于MNMT来说，额外的参数并没有被完全优化以避免脱靶问题。
D我们方法的潜在风险
由于我们提出的方法可以增加无限数量的翻译方向，因此可能会有一些恶意用户使用MNMT模型为政治敏感语言提供翻译服务。例如，恶意用户可能会利用我们的模型在一些政治敏感语言中生成仇恨或冒犯性的句子。
